#!/usr/bin/env python3
"""
æµ‹è¯•ä¿®æ­£åçš„Llama FLOPSå’Œå†…å­˜è®¡ç®—

éªŒè¯åŸºäºmeta-llamaå®˜æ–¹å®ç°çš„ä¿®æ­£è®¡ç®—å…¬å¼
"""

import sys
import os

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
sys.path.insert(0, os.path.join(os.path.dirname(__file__), '..', '..'))

from llm_estimate.models.registry import model_registry

# å¯é€‰å¯¼å…¥pytest
try:
    import pytest
    PYTEST_AVAILABLE = True
except ImportError:
    PYTEST_AVAILABLE = False
    print("è­¦å‘Š: pytestæœªå®‰è£…ï¼Œè·³è¿‡å•å…ƒæµ‹è¯•ç±»")


if PYTEST_AVAILABLE:
    class TestLlamaCorrections:
        """æµ‹è¯•Llamaæ¨¡å‹FLOPSå’Œå†…å­˜è®¡ç®—çš„ä¿®æ­£"""
        
        @pytest.fixture
        def test_models(self):
            """æµ‹è¯•æ¨¡å‹åˆ—è¡¨"""
            return [
                "llama-2-7b",
                "llama-3.1-8b",
            ]
        
        def test_model_creation(self, test_models):
            """æµ‹è¯•æ¨¡å‹åˆ›å»º"""
            for model_name in test_models:
                model = model_registry.create_model(model_name)
                assert model is not None
                assert model.specs.name == model_name
        
        def test_flops_calculation(self, test_models):
            """æµ‹è¯•FLOPSè®¡ç®—"""
            for model_name in test_models:
                model = model_registry.create_model(model_name)
                
                # è¯¦ç»†FLOPSè®¡ç®—
                detailed_flops = model.estimate_flops_per_token()
                assert detailed_flops > 0
                
                # ç®€åŒ–FLOPSè®¡ç®— (OpenAI scaling laws)
                simplified_flops = model.estimate_model_flops_per_token_simplified()
                assert simplified_flops > 0
                
                # ä¸¤ç§è®¡ç®—ç»“æœåº”è¯¥åœ¨åŒä¸€ä¸ªæ•°é‡çº§
                ratio = detailed_flops / simplified_flops
                assert 0.5 <= ratio <= 3.0, f"FLOPSè®¡ç®—æ¯”ç‡å¼‚å¸¸: {ratio}"
        
        def test_memory_calculation(self, test_models):
            """æµ‹è¯•å†…å­˜è®¡ç®—"""
            for model_name in test_models:
                model = model_registry.create_model(model_name)
                
                memory_usage = model.calculate_memory_usage("fp16")
                
                # æ£€æŸ¥æ‰€æœ‰å†…å­˜ç»„ä»¶éƒ½å¤§äº0
                assert memory_usage["model_memory_gb"] > 0
                assert memory_usage["kv_cache_memory_gb"] >= 0  # å¯èƒ½ä¸º0ï¼ˆå¦‚æœç¦ç”¨KVç¼“å­˜ï¼‰
                assert memory_usage["total_activation_memory_gb"] > 0
                assert memory_usage["total_inference_gb"] > 0
                assert memory_usage["total_training_gb"] > 0
                
                # è®­ç»ƒå†…å­˜åº”è¯¥å¤§äºæ¨ç†å†…å­˜
                assert memory_usage["total_training_gb"] > memory_usage["total_inference_gb"]
        
        def test_gqa_optimization(self):
            """æµ‹è¯•GQAä¼˜åŒ–çš„æ•ˆæœ"""
            # æ¯”è¾ƒç›¸ä¼¼è§„æ¨¡çš„MHAå’ŒGQAæ¨¡å‹
            model_mha = model_registry.create_model("llama-2-7b")  # MHA
            model_gqa = model_registry.create_model("llama-3.1-8b")  # GQA
            
            # ä½¿ç”¨ç›¸åŒé…ç½®
            config = {"context_length": 4096, "batch_size": 1, "precision": "fp16"}
            model_mha.update_config(**config)
            model_gqa.update_config(**config)
            
            # è®¡ç®—KVç¼“å­˜ä½¿ç”¨
            mem_mha = model_mha.calculate_memory_usage()
            mem_gqa = model_gqa.calculate_memory_usage()
            
            kv_mha = mem_mha['kv_cache_memory_gb']
            kv_gqa = mem_gqa['kv_cache_memory_gb']
            
            # GQAåº”è¯¥æ˜¾è‘—å‡å°‘KVç¼“å­˜ä½¿ç”¨
            assert kv_gqa < kv_mha, "GQAåº”è¯¥å‡å°‘KVç¼“å­˜ä½¿ç”¨"
            
            reduction = ((kv_mha - kv_gqa) / kv_mha) * 100
            assert reduction > 50, f"GQA KVç¼“å­˜èŠ‚çœåº”è¯¥è¶…è¿‡50%ï¼Œå®é™…: {reduction:.1f}%"
        
        def test_context_length_scaling(self):
            """æµ‹è¯•ä¸Šä¸‹æ–‡é•¿åº¦å¯¹å†…å­˜çš„å½±å“"""
            model = model_registry.create_model("llama-3.1-8b")
            
            context_lengths = [1024, 4096, 8192, 16384]
            kv_cache_sizes = []
            
            for ctx_len in context_lengths:
                model.update_config(context_length=ctx_len)
                memory_usage = model.calculate_memory_usage()
                kv_cache_sizes.append(memory_usage['kv_cache_memory_gb'])
            
            # KVç¼“å­˜åº”è¯¥éšä¸Šä¸‹æ–‡é•¿åº¦çº¿æ€§å¢é•¿
            for i in range(1, len(context_lengths)):
                ratio_ctx = context_lengths[i] / context_lengths[i-1]
                ratio_mem = kv_cache_sizes[i] / kv_cache_sizes[i-1]
                
                # å…è®¸ä¸€å®šçš„è¯¯å·®èŒƒå›´
                assert abs(ratio_ctx - ratio_mem) < 0.1, f"KVç¼“å­˜ä¸æ˜¯çº¿æ€§ç¼©æ”¾: {ratio_ctx} vs {ratio_mem}"
        
        def test_precision_scaling(self):
            """æµ‹è¯•ä¸åŒç²¾åº¦å¯¹å†…å­˜çš„å½±å“"""
            model = model_registry.create_model("llama-2-7b")
            
            precisions = ["fp32", "fp16", "int8", "int4"]
            expected_bytes = [4, 2, 1, 0.5]
            
            memory_usages = []
            for precision in precisions:
                memory_usage = model.calculate_memory_usage(precision)
                memory_usages.append(memory_usage['model_memory_gb'])
            
            # å†…å­˜ä½¿ç”¨åº”è¯¥æŒ‰ç²¾åº¦æ¯”ä¾‹ç¼©æ”¾
            for i in range(1, len(precisions)):
                expected_ratio = expected_bytes[0] / expected_bytes[i]
                actual_ratio = memory_usages[0] / memory_usages[i]
                
                # å…è®¸5%çš„è¯¯å·®
                assert abs(expected_ratio - actual_ratio) / expected_ratio < 0.05, \
                    f"ç²¾åº¦ {precisions[i]} å†…å­˜ç¼©æ”¾ä¸æ­£ç¡®: é¢„æœŸ {expected_ratio}, å®é™… {actual_ratio}"
        
        def test_performance_analysis(self):
            """æµ‹è¯•æ€§èƒ½åˆ†æåŠŸèƒ½"""
            model = model_registry.create_model("llama-2-7b")
            analysis = model.get_performance_analysis()
            
            # æ£€æŸ¥åˆ†æç»“æœç»“æ„
            assert "model_info" in analysis
            assert "flops_analysis" in analysis
            assert "memory_analysis" in analysis
            assert "efficiency_metrics" in analysis
            
            # æ£€æŸ¥FLOPSåˆ†å¸ƒåˆç†æ€§
            flops_info = analysis["flops_analysis"]
            attn_pct = flops_info["attention_flops_percentage"]
            ffn_pct = flops_info["ffn_flops_percentage"]
            
            # æ³¨æ„åŠ›å’ŒFFNåº”è¯¥å æ®å¤§éƒ¨åˆ†FLOPS
            assert attn_pct + ffn_pct > 80, "æ³¨æ„åŠ›å’ŒFFNåº”è¯¥å æ®å¤§éƒ¨åˆ†FLOPS"
            
            # FFNé€šå¸¸æ¯”æ³¨æ„åŠ›æ¶ˆè€—æ›´å¤šFLOPSï¼ˆç‰¹åˆ«æ˜¯å¯¹äºè¾ƒå¤§çš„æ¨¡å‹ï¼‰
            if model.specs.parameters >= 13:  # 13BåŠä»¥ä¸Šæ¨¡å‹
                assert ffn_pct > attn_pct, "å¯¹äºå¤§æ¨¡å‹ï¼ŒFFN FLOPSåº”è¯¥å¤§äºæ³¨æ„åŠ›FLOPS"


def run_interactive_test():
    """äº¤äº’å¼æµ‹è¯•å‡½æ•°ï¼Œç”¨äºæ‰‹åŠ¨è¿è¡Œå’ŒæŸ¥çœ‹è¯¦ç»†ç»“æœ"""
    
    test_models = [
        "llama-2-7b",
        "llama-3.1-8b",
    ]
    
    print("=" * 80)
    print("Llamaæ¨¡å‹FLOPSå’Œå†…å­˜è®¡ç®—æµ‹è¯• (åŸºäºmeta-llamaå®˜æ–¹å®ç°ä¿®æ­£)")
    print("=" * 80)
    
    for model_name in test_models:
        try:
            print(f"\nğŸ” æµ‹è¯•æ¨¡å‹: {model_name}")
            print("-" * 50)
            
            # åˆ›å»ºæ¨¡å‹å®ä¾‹
            model = model_registry.create_model(model_name)
            
            # è·å–å®Œæ•´æ€§èƒ½åˆ†æ
            analysis = model.get_performance_analysis()
            
            # æ˜¾ç¤ºæ¨¡å‹åŸºæœ¬ä¿¡æ¯
            model_info = analysis["model_info"]
            print(f"ğŸ“Š æ¨¡å‹ä¿¡æ¯:")
            print(f"   å‚æ•°é‡: {model_info['parameters']}")
            print(f"   å±‚æ•°: {model_info['layers']}")
            print(f"   éšè—ç»´åº¦: {model_info['hidden_size']}")
            print(f"   æ³¨æ„åŠ›å¤´æ•°: {model_info['attention_heads']}")
            print(f"   KVå¤´æ•°: {model_info['kv_heads']} (GQAæ¯”ç‡: {model_info['attention_heads']//model_info['kv_heads']:.1f})")
            
            # FLOPSåˆ†æ
            flops_info = analysis["flops_analysis"]
            print(f"\nâš¡ FLOPSåˆ†æ (æ¯token):")
            print(f"   æ€»FLOPS: {flops_info['total_flops_per_token']:,.0f}")
            print(f"   ç®€åŒ–FLOPS (OpenAI): {flops_info['simplified_flops_per_token']:,.0f}")
            print(f"   æ³¨æ„åŠ›FLOPS: {flops_info['attention_flops_per_token']:,.0f} ({flops_info['attention_flops_percentage']:.1f}%)")
            print(f"   FFN FLOPS: {flops_info['ffn_flops_per_token']:,.0f} ({flops_info['ffn_flops_percentage']:.1f}%)")
            
            # å†…å­˜åˆ†æ
            memory_info = analysis["memory_analysis"]
            print(f"\nğŸ’¾ å†…å­˜ä½¿ç”¨ (ç²¾åº¦: {memory_info['precision']}):")
            print(f"   æ¨¡å‹æƒé‡: {memory_info['model_memory_gb']:.2f} GB")
            print(f"   KVç¼“å­˜: {memory_info['kv_cache_memory_gb']:.2f} GB")
            print(f"   æ³¨æ„åŠ›åˆ†æ•°: {memory_info['attention_scores_memory_gb']:.2f} GB")
            print(f"   æ€»æ¿€æ´»å€¼: {memory_info['total_activation_memory_gb']:.2f} GB")
            print(f"   æ¨ç†æ€»å†…å­˜: {memory_info['total_inference_gb']:.2f} GB")
            print(f"   è®­ç»ƒæ€»å†…å­˜: {memory_info['total_training_gb']:.2f} GB")
            
            # æ•ˆç‡æŒ‡æ ‡
            efficiency = analysis["efficiency_metrics"]
            print(f"\nğŸ“ˆ æ•ˆç‡æŒ‡æ ‡:")
            print(f"   æ¯å‚æ•°å†…å­˜: {efficiency['memory_per_parameter_bytes']:.2f} bytes")
            print(f"   æ¯å‚æ•°FLOPS: {efficiency['flops_per_parameter']:.0f}")
            print(f"   KVç¼“å­˜å¼€é”€: {efficiency['kv_cache_overhead_percentage']:.1f}%")
            
            # ä¸Šä¸‹æ–‡é•¿åº¦åˆ†æ (ä»…å¯¹æ”¯æŒé•¿ä¸Šä¸‹æ–‡çš„æ¨¡å‹)
            if model_name.startswith("llama-3.1"):
                print(f"\nğŸ”„ é•¿ä¸Šä¸‹æ–‡åˆ†æ:")
                print("   æµ‹è¯•ä¸åŒä¸Šä¸‹æ–‡é•¿åº¦çš„KVç¼“å­˜ä½¿ç”¨...")
                
                for ctx_len in [4096, 8192, 32768, 131072]:
                    model.update_config(context_length=ctx_len)
                    mem_usage = model.calculate_memory_usage()
                    kv_cache_gb = mem_usage['kv_cache_memory_gb']
                    print(f"     {ctx_len:>6} tokens: KVç¼“å­˜ {kv_cache_gb:.2f} GB")
                
                # æ¢å¤é»˜è®¤ä¸Šä¸‹æ–‡é•¿åº¦
                model.update_config(context_length=4096)
            
        except Exception as e:
            print(f"âŒ é”™è¯¯: {e}")
            continue
    
    # æ¯”è¾ƒæ³¨æ„åŠ›æœºåˆ¶
    print("\n" + "=" * 80)
    print("æ³¨æ„åŠ›æœºåˆ¶æ•ˆç‡å¯¹æ¯” (MHA vs GQA)")
    print("=" * 80)
    
    comparisons = [
        ("llama-2-7b", "llama-3.1-8b"),   # 7B vs 8B
    ]
    
    for mha_model, gqa_model in comparisons:
        print(f"\nğŸ”„ å¯¹æ¯”: {mha_model} (MHA) vs {gqa_model} (GQA)")
        print("-" * 60)
        
        try:
            model_mha = model_registry.create_model(mha_model)
            model_gqa = model_registry.create_model(gqa_model)
            
            config = {"context_length": 4096, "batch_size": 1, "precision": "fp16"}
            model_mha.update_config(**config)
            model_gqa.update_config(**config)
            
            mem_mha = model_mha.calculate_memory_usage()
            mem_gqa = model_gqa.calculate_memory_usage()
            
            kv_mha = mem_mha['kv_cache_memory_gb']
            kv_gqa = mem_gqa['kv_cache_memory_gb']
            
            reduction = ((kv_mha - kv_gqa) / kv_mha) * 100
            
            print(f"   MHA KVç¼“å­˜: {kv_mha:.2f} GB")
            print(f"   GQA KVç¼“å­˜: {kv_gqa:.2f} GB")
            print(f"   å†…å­˜èŠ‚çœ: {reduction:.1f}%")
            
        except Exception as e:
            print(f"   âŒ å¯¹æ¯”å¤±è´¥: {e}")
    
    print("\n" + "=" * 80)
    print("âœ… æµ‹è¯•å®Œæˆ!")
    print("=" * 80)


if __name__ == "__main__":
    # å¦‚æœç›´æ¥è¿è¡Œæ­¤æ–‡ä»¶ï¼Œæ‰§è¡Œäº¤äº’å¼æµ‹è¯•
    run_interactive_test() 