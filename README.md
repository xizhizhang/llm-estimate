# LLM-Estimate

å¤§è¯­è¨€æ¨¡å‹æ€§èƒ½ä¼°ç®—å·¥å…· - ä¼°ç®—LLMåœ¨ä¸åŒç¡¬ä»¶é…ç½®ä¸‹çš„æ€§èƒ½è¡¨ç°ã€‚

## åŠŸèƒ½ç‰¹æ€§

- ğŸš€ æ”¯æŒå¤šç§ä¸»æµLLMæ¨¡å‹ï¼ˆLlamaã€Qwenç­‰ï¼‰
- ğŸ’» ç»Ÿä¸€åŠ é€Ÿå™¨æŠ½è±¡ï¼ˆGPUã€CPUã€TPUã€NPUç­‰ï¼‰
- ğŸ“Š ä¼°ç®—å…³é”®æ€§èƒ½æŒ‡æ ‡ï¼ˆååé‡ã€å»¶è¿Ÿã€å†…å­˜ä½¿ç”¨ï¼‰
- ğŸ”§ æä¾›ä¼˜åŒ–å»ºè®®å’Œç“¶é¢ˆåˆ†æ
- ğŸ“‹ æ”¯æŒå¤šç§è¾“å‡ºæ ¼å¼ï¼ˆè¡¨æ ¼ã€JSONã€CSVï¼‰
- ğŸ–¥ï¸ å‘½ä»¤è¡Œå·¥å…·å’ŒPython API
- âš¡ ä¸“æ³¨ç®—åŠ›ï¼ˆFLOPSï¼‰å’Œå†…å­˜å¸¦å®½æ ¸å¿ƒæŒ‡æ ‡

## æ ¸å¿ƒæ¦‚å¿µ

æœ¬é¡¹ç›®å°†GPUã€CPUã€TPUç­‰è®¡ç®—è®¾å¤‡ç»Ÿä¸€æŠ½è±¡ä¸º**åŠ é€Ÿå™¨**ï¼Œä¸å†åŒºåˆ†è®¾å¤‡ç±»å‹ï¼Œåªå…³æ³¨ï¼š
- **ç®—åŠ›**: è®¡ç®—èƒ½åŠ›ï¼ˆTFLOPSï¼‰
- **å†…å­˜å¸¦å®½**: å­˜å‚¨å¸¦å®½ï¼ˆGB/sï¼‰
- **å†…å­˜å®¹é‡**: å¯ç”¨å†…å­˜ï¼ˆGBï¼‰

è¿™ç§ç»Ÿä¸€æŠ½è±¡ç®€åŒ–äº†ç¡¬ä»¶é…ç½®ï¼Œä½¿æ€§èƒ½ä¼°ç®—æ›´åŠ ç›´è§‚å’Œå‡†ç¡®ã€‚

## å¿«é€Ÿå¼€å§‹

### å®‰è£…

```bash
# å…‹éš†é¡¹ç›®
git clone https://github.com/zhangwm/llm-estimate.git
cd llm-estimate

# å®‰è£…ä¾èµ–
pip install -r requirements.txt

# å¯é€‰ï¼šå®‰è£…é¡¹ç›®ï¼ˆç”¨äºå…¨å±€å‘½ä»¤ï¼‰
pip install -e .
```

### è¿è¡Œæ–¹å¼

#### æ–¹å¼1: ç›´æ¥è¿è¡Œï¼ˆæ¨èï¼Œæ— éœ€å®‰è£…ï¼‰

```bash
# ä½¿ç”¨é¡¹ç›®æ ¹ç›®å½•çš„å…¥å£è„šæœ¬
python3 llm_estimate.py --help

# æˆ–è€…ç»™è„šæœ¬æ‰§è¡Œæƒé™åç›´æ¥è¿è¡Œ
chmod +x llm_estimate.py
./llm_estimate.py --help
```

#### æ–¹å¼2: æ¨¡å—æ–¹å¼è¿è¡Œ

```bash
# åœ¨é¡¹ç›®æ ¹ç›®å½•ä¸‹è¿è¡ŒCLIæ¨¡å—
python3 -m llm_estimate.cli --help

# æˆ–è€…è¿è¡Œæ¨¡å—å†…çš„main.py
python3 llm_estimate/main.py --help
```

#### æ–¹å¼3: å®‰è£…åå…¨å±€å‘½ä»¤

```bash
# å…ˆå®‰è£…é¡¹ç›®
pip install -e .

# ç„¶åå¯ä»¥å…¨å±€ä½¿ç”¨
llm-estimate --help
```

**æ³¨æ„**: æ–¹å¼1æ˜¯æœ€ç®€å•çš„æ–¹æ³•ï¼Œåªéœ€è¦å…‹éš†é¡¹ç›®å’Œå®‰è£…ä¾èµ–å³å¯è¿è¡Œï¼Œæ— éœ€å®‰è£…åŒ…ã€‚

### åŸºæœ¬ä½¿ç”¨

#### å‘½ä»¤è¡Œå·¥å…·

```bash
# ä¼°ç®—Llama-2-7Båœ¨RTX-4090ä¸Šçš„æ€§èƒ½
python3 llm_estimate.py estimate --model llama-2-7b --accelerator rtx-4090

# ä½¿ç”¨å¤šä¸ªåŠ é€Ÿå™¨
python3 llm_estimate.py estimate --model llama-2-7b --accelerators rtx-4090,a100-40gb

# æŒ‡å®šç²¾åº¦å’Œæ‰¹æ¬¡å¤§å°
python3 llm_estimate.py estimate --model llama-2-7b --accelerator rtx-4090 --precision fp16 --batch-size 4

# åˆ—å‡ºæ”¯æŒçš„æ¨¡å‹
python3 llm_estimate.py list-models

# åˆ—å‡ºæ”¯æŒçš„åŠ é€Ÿå™¨
python3 llm_estimate.py list-accelerators

# æŒ‰ç±»å‹ç­›é€‰åŠ é€Ÿå™¨
python3 llm_estimate.py list-accelerators --type gpu
python3 llm_estimate.py list-accelerators --type cpu

# æ¯”è¾ƒå¤šä¸ªæ¨¡å‹
python3 llm_estimate.py compare --models llama-2-7b,qwen-7b --accelerator rtx-4090

# åŸºå‡†æµ‹è¯•å¤šä¸ªåŠ é€Ÿå™¨
python3 llm_estimate.py benchmark --accelerators rtx-4090,a100-40gb,h100 --model llama-2-7b

# äº¤äº’å¼æ¨¡å¼
python3 llm_estimate.py interactive
```

#### Python API

```python
from llm_estimate import PerformanceEstimator, create_accelerator

# åˆ›å»ºä¼°ç®—å™¨
estimator = PerformanceEstimator()

# å•åŠ é€Ÿå™¨ä¼°ç®—
result = estimator.estimate(
    model_name="llama-2-7b",
    hardware_config={"accelerator": "rtx-4090"},
    model_config={"batch_size": 1, "precision": "fp16"}
)

# å¤šåŠ é€Ÿå™¨ä¼°ç®—
result = estimator.estimate(
    model_name="llama-2-7b",
    hardware_config={"accelerators": ["rtx-4090", "a100-40gb"]},
    model_config={"batch_size": 4, "precision": "fp16"}
)

print(f"ååé‡: {result['throughput_tokens_per_sec']:.1f} tokens/s")
print(f"å†…å­˜ä½¿ç”¨: {result['memory_usage_gb']:.2f} GB")
print(f"å»¶è¿Ÿ: {result['latency_ms']:.1f} ms")
print(f"ç“¶é¢ˆ: {result['bottleneck']}")

# ç›´æ¥åˆ›å»ºåŠ é€Ÿå™¨
accelerator = create_accelerator("rtx-4090")
print(f"ç®—åŠ›: {accelerator.compute_capability_tflops} TFLOPS")
print(f"å†…å­˜å¸¦å®½: {accelerator.memory_bandwidth_gb_s} GB/s")
```

## é¡¹ç›®ç»“æ„

```
llm-estimate/
â”œâ”€â”€ llm_estimate/           # ä¸»è¦ä»£ç ç›®å½•
â”‚   â”œâ”€â”€ models/            # æ¨¡å‹ç®¡ç†æ¨¡å—
â”‚   â”œâ”€â”€ hardware/          # ç¡¬ä»¶ç®¡ç†æ¨¡å—ï¼ˆç»Ÿä¸€åŠ é€Ÿå™¨ï¼‰
â”‚   â”œâ”€â”€ estimator/         # ä¼°ç®—å¼•æ“æ¨¡å—
â”‚   â”œâ”€â”€ config/            # å…¨å±€é…ç½®æ¨¡å—
â”‚   â”œâ”€â”€ utils/             # å·¥å…·æ¨¡å—
â”‚   â””â”€â”€ cli/               # å‘½ä»¤è¡Œæ¥å£
â”œâ”€â”€ tests/                 # æµ‹è¯•æ¨¡å—
â”œâ”€â”€ data/                  # æ•°æ®ç›®å½•
â”œâ”€â”€ docs/                  # æ–‡æ¡£ç›®å½•
â””â”€â”€ scripts/               # è„šæœ¬ç›®å½•
```

## æ”¯æŒçš„æ¨¡å‹

- **Llamaç³»åˆ—**: Llama-2-7B, Llama-2-13B, Llama-2-70B
- **Qwenç³»åˆ—**: Qwen-7B, Qwen-14B, Qwen-72B
- æ›´å¤šæ¨¡å‹æŒç»­æ·»åŠ ä¸­...

## æ”¯æŒçš„åŠ é€Ÿå™¨

### GPUåŠ é€Ÿå™¨
- **NVIDIA**: RTX-4090, RTX-4080, RTX-3090, A100, H100, V100
- **AMD**: (è§„åˆ’ä¸­)

### CPUåŠ é€Ÿå™¨
- **Intel**: i9-13900K, i7-13700K
- **AMD**: Ryzen-9-7950X

### ä¸“ç”¨åŠ é€Ÿå™¨
- **Apple**: M1-Ultra, M2-Ultra
- **Google**: TPU-v4

## å…¼å®¹æ€§è¯´æ˜

ä¸ºä¿æŒå‘åå…¼å®¹ï¼Œä»æ”¯æŒæ—§çš„`--gpu`å’Œ`--cpu`å‚æ•°ï¼Œä½†å»ºè®®ä½¿ç”¨æ–°çš„`--accelerator`å‚æ•°ã€‚

```bash
# æ—§æ ¼å¼ï¼ˆä»ç„¶æ”¯æŒï¼‰
llm-estimate estimate --model llama-2-7b --gpu rtx-4090

# æ–°æ ¼å¼ï¼ˆæ¨èï¼‰
llm-estimate estimate --model llama-2-7b --accelerator rtx-4090
```

## å¼€å‘

### ç¯å¢ƒè®¾ç½®

```bash
# å®‰è£…å¼€å‘ä¾èµ–
pip install -e ".[dev]"

# è¿è¡Œæµ‹è¯•
pytest

# ä»£ç æ ¼å¼åŒ–
black llm_estimate/

# ç±»å‹æ£€æŸ¥
mypy llm_estimate/
```

### æ·»åŠ æ–°æ¨¡å‹

1. åœ¨ `llm_estimate/models/` ä¸­åˆ›å»ºæ–°çš„æ¨¡å‹ç±»
2. ç»§æ‰¿ `BaseModel` å¹¶å®ç°å¿…è¦æ–¹æ³•
3. åœ¨ `registry.py` ä¸­æ³¨å†Œæ–°æ¨¡å‹

### æ·»åŠ æ–°åŠ é€Ÿå™¨

1. åœ¨ `llm_estimate/hardware/accelerator.py` çš„ `ACCELERATOR_SPECS` ä¸­æ·»åŠ è§„æ ¼
2. æä¾›ç®—åŠ›ï¼ˆTFLOPSï¼‰ã€å†…å­˜å¸¦å®½ï¼ˆGB/sï¼‰ã€å†…å­˜å®¹é‡ï¼ˆGBï¼‰ç­‰å…³é”®å‚æ•°
3. å¯é€‰æ‹©æ€§æ·»åŠ åŠŸè€—ã€ä»·æ ¼ç­‰è¾…åŠ©ä¿¡æ¯

ç¤ºä¾‹ï¼š
```python
"new-accelerator": AcceleratorSpecs(
    name="New-Accelerator",
    manufacturer="Vendor",
    device_type="gpu",  # æˆ– "cpu", "tpu", "soc"
    compute_capability_tflops=100.0,
    memory_bandwidth_gb_s=1500.0,
    memory_capacity_gb=48.0,
    release_year=2024,
    price_usd=5000,
    power_consumption_w=400
)
```

## è®¸å¯è¯

MIT License

## è´¡çŒ®

æ¬¢è¿æäº¤Issueå’ŒPull Requestï¼
